{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = os.path.join('./Dataset/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_number(string):\n",
    "    return int(string.split('/')[2].split('.')[0])\n",
    "file_list.sort(key=file_number)\n",
    "lemmat = WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(Inverted_Positional_Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvertedPositionalIndex = dict()\n",
    "Inverted_Positional_Dictionary = dict()\n",
    "Document_term_dictionary = dict()\n",
    "for i in range(0,len(file_list)):\n",
    "    lt = file_list[i]\n",
    "    with open(lt) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        stp = (soup.text).replace('\\n',' ').lower().strip()\n",
    "        new_words = tokenizer.tokenize(stp)\n",
    "        DF = {}\n",
    "        for token in new_words:\n",
    "            if token not in stop:\n",
    "                token = lemmat.lemmatize(token)\n",
    "                if token in DF.keys():\n",
    "                    DF[token] += 1\n",
    "                else:\n",
    "                    DF[token] = 1\n",
    "    for key,value in DF.items():\n",
    "        t = (key,0)\n",
    "        if key not in InvertedPositionalIndex.keys():\n",
    "            InvertedPositionalIndex[t] = list()\n",
    "        InvertedPositionalIndex[t].append((file_number(lt),np.log10(1+value)))\n",
    "        if key not in Inverted_Positional_Dictionary.keys():\n",
    "            Inverted_Positional_Dictionary[key] = list()\n",
    "        Inverted_Positional_Dictionary[key].append((file_number(lt),np.log10(1+value)))\n",
    "    for key,value in DF.items():\n",
    "        DF[key] = np.log10(1+value)\n",
    "    Document_term_dictionary[file_number(lt)] = DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChampionListLocal = dict()\n",
    "for key,value in Inverted_Positional_Dictionary.items():\n",
    "    value = sorted(value, key = lambda x: x[1],reverse=True)\n",
    "    Inverted_Positional_Dictionary[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Inverted_Positional_Dictionary.items():\n",
    "    ChampionListLocal[key] = list()\n",
    "    for i in range(0,50):\n",
    "        if(i<len(value)):\n",
    "            ChampionListLocal[key].append(value[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChampionListGlobal = dict()\n",
    "with open('StaticQualityScore.pkl','rb') as f:\n",
    "    Stat_scores = pickle.load(f)\n",
    "for key,value in Inverted_Positional_Dictionary.items():\n",
    "    value = sorted(value, key = lambda x: (x[1]+Stat_scores[x[0]]),reverse=True)\n",
    "    Inverted_Positional_Dictionary[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Inverted_Positional_Dictionary.items():\n",
    "    ChampionListGlobal[key] = list()\n",
    "    for i in range(0,50):\n",
    "        if(i<len(value)):\n",
    "            ChampionListGlobal[key].append(value[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"sony japan inc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizequery(K,query,position_terms,Inverted_Positional_Dictionary):\n",
    "    query = query.lower().strip()\n",
    "    q_words = tokenizer.tokenize(query)\n",
    "    y = np.zeros(K)\n",
    "    for q in q_words:\n",
    "        if q not in stop:\n",
    "            q = lemmat.lemmatize(q)\n",
    "            if q in Inverted_Positional_Dictionary.keys():\n",
    "                y[position_terms[q]] = np.log10(1000/len(Inverted_Positional_Dictionary[q]))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_terms = dict()\n",
    "terms = list(Inverted_Positional_Dictionary.keys())\n",
    "for i in range(0,len(terms)):\n",
    "    position_terms[terms[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0457574905606752"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(1000/9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizedoc(K,doc,position_terms,Inverted_Positional_Dictionary,Document_term_dictionary):\n",
    "    dit = Document_term_dictionary[doc]\n",
    "    y = np.zeros(K)\n",
    "    for key,value in dit.items():\n",
    "        y[position_terms[key]] = value*(np.log10(1000/len(Inverted_Positional_Dictionary[key])))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [0,1,3,4]\n",
    "q = [1,2,0,4]\n",
    "np.dot(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0990195135927845"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = dict()\n",
    "q = vectorizequery(K,query,position_terms,Inverted_Positional_Dictionary)\n",
    "p = list()\n",
    "for i in range(0,1000):\n",
    "    p.append(list())\n",
    "for i in range(0,1000):\n",
    "    p[i] = vectorizedoc(K,i,position_terms,Inverted_Positional_Dictionary,Document_term_dictionary)\n",
    "    relevance[i] = (np.dot(p[i],q))/(np.linalg.norm(p[i])*np.linalg.norm(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = sorted(relevance.items(), key=lambda item: item[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = res1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sony japan inc\n",
      "<761,0.04163736936684436>,<803,0.0384827526498095>,<353,0.03818757710745008>,<244,0.02809288494765068>,<346,0.027127485458486093>,<136,0.025740466941402907>,<919,0.02407147997530029>,<322,0.02068334218218271>,<952,0.02035710302239123>,<797,0.019743619877684325>\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "for i in range(0,len(res1)):\n",
    "    if(i+1<10):\n",
    "        print(\"<\"+str(res1[i][0])+\",\"+str(res1[i][1])+\">\",end=\",\")\n",
    "    else:\n",
    "        print(\"<\"+str(res1[i][0])+\",\"+str(res1[i][1])+\">\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_champion_local_docs(K,query,position_terms,ChampionListLocal):\n",
    "    query = query.lower().strip()\n",
    "    q_words = tokenizer.tokenize(query)\n",
    "    y = list()\n",
    "    for q in q_words:\n",
    "        if q not in stop:\n",
    "            q = lemmat.lemmatize(q)\n",
    "            if q in ChampionListLocal.keys():\n",
    "                y.extend(ChampionListLocal[q])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_champion_local_docs(K,query,position_terms,ChampionListLocal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BuilBuilding InvertedPositionalIndex\n",
      "Building ChampionListLocal\n",
      "Building ChampionListGlobal\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def file_number(string):\n",
    "    return int(string.split('/')[2].split('.')[0])\n",
    "\n",
    "pth = os.path.join('./Dataset/*.html')\n",
    "file_list = glob.glob(pth)\n",
    "file_list.sort(key=file_number)\n",
    "lemmat = WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "def vectorizequery(K,query,position_terms,Inverted_Positional_Dictionary):\n",
    "    query = query.lower().strip()\n",
    "    q_words = tokenizer.tokenize(query)\n",
    "    y = np.zeros(K)\n",
    "    for q in q_words:\n",
    "        if q not in stop:\n",
    "            q = lemmat.lemmatize(q)\n",
    "            if q in Inverted_Positional_Dictionary.keys():\n",
    "                y[position_terms[q]] = np.log10(1000/len(Inverted_Positional_Dictionary[q]))\n",
    "    return y\n",
    "\n",
    "def get_champion_local_docs(K,query,position_terms,ChampionListLocal):\n",
    "    query = query.lower().strip()\n",
    "    q_words = tokenizer.tokenize(query)\n",
    "    y = list()\n",
    "    for q in q_words:\n",
    "        if q not in stop:\n",
    "            q = lemmat.lemmatize(q)\n",
    "            if q in ChampionListLocal.keys():\n",
    "                y.extend(ChampionListLocal[q])\n",
    "    return y\n",
    "\n",
    "def get_champion_global_docs(K,query,position_terms,ChampionListGlobal):\n",
    "    query = query.lower().strip()\n",
    "    q_words = tokenizer.tokenize(query)\n",
    "    y = list()\n",
    "    for q in q_words:\n",
    "        if q not in stop:\n",
    "            q = lemmat.lemmatize(q)\n",
    "            if q in ChampionListGlobal.keys():\n",
    "                y.extend(ChampionListGlobal[q])\n",
    "    return y\n",
    "\n",
    "def vectorizedoc(K,doc,position_terms,Inverted_Positional_Dictionary,Document_term_dictionary):\n",
    "    dit = Document_term_dictionary[doc]\n",
    "    y = np.zeros(K)\n",
    "    for key,value in dit.items():\n",
    "        y[position_terms[key]] = value*(np.log10(1000/len(Inverted_Positional_Dictionary[key])))\n",
    "    return y\n",
    "\n",
    "############## Building InvertedPositionalIndex ##################\n",
    "print(\"BuilBuilding InvertedPositionalIndex\")\n",
    "InvertedPositionalIndex = dict()\n",
    "Inverted_Positional_Dictionary = dict()\n",
    "Document_term_dictionary = dict()\n",
    "for i in range(0,len(file_list)):\n",
    "    lt = file_list[i]\n",
    "    with open(lt) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        stp = (soup.text).replace('\\n',' ').lower().strip()\n",
    "        new_words = tokenizer.tokenize(stp)\n",
    "        DF = {}\n",
    "        for token in new_words:\n",
    "            if token in DF.keys():\n",
    "                DF[token] += 1\n",
    "            else:\n",
    "                DF[token] = 1\n",
    "    for key,value in DF.items():\n",
    "        t = (key,0)\n",
    "        if key not in InvertedPositionalIndex.keys():\n",
    "            InvertedPositionalIndex[t] = list()\n",
    "        InvertedPositionalIndex[t].append((file_number(lt),np.log10(1+value)))\n",
    "        if key not in Inverted_Positional_Dictionary.keys():\n",
    "            Inverted_Positional_Dictionary[key] = list()\n",
    "        Inverted_Positional_Dictionary[key].append((file_number(lt),np.log10(1+value)))\n",
    "    for key,value in DF.items():\n",
    "        DF[key] = np.log10(1+value)\n",
    "    Document_term_dictionary[file_number(lt)] = DF\n",
    "\n",
    "############## Building ChampionListLocal ##################\n",
    "print(\"Building ChampionListLocal\")\n",
    "ChampionListLocal = dict()\n",
    "for key,value in Inverted_Positional_Dictionary.items():\n",
    "    value = sorted(value, key = lambda x: x[1],reverse=True)\n",
    "    Inverted_Positional_Dictionary[key] = value\n",
    "for key,value in Inverted_Positional_Dictionary.items():\n",
    "    ChampionListLocal[key] = list()\n",
    "    for i in range(0,50):\n",
    "        if(i<len(value)):\n",
    "            ChampionListLocal[key].append(value[i][0])\n",
    "\n",
    "############## Building ChampionListGlobal ##################\n",
    "print(\"Building ChampionListGlobal\")\n",
    "ChampionListGlobal = dict()\n",
    "with open('StaticQualityScore.pkl','rb') as f:\n",
    "    Stat_scores = pickle.load(f)\n",
    "for key,value in Inverted_Positional_Dictionary.items():\n",
    "    value = sorted(value, key = lambda x: (x[1]+Stat_scores[x[0]]),reverse=True)\n",
    "    Inverted_Positional_Dictionary[key] = value\n",
    "for key,value in Inverted_Positional_Dictionary.items():\n",
    "    ChampionListGlobal[key] = list()\n",
    "    for i in range(0,50):\n",
    "        if(i<len(value)):\n",
    "            ChampionListGlobal[key].append(value[i][0])\n",
    "\n",
    "############## Answering Free Text Queries ######################\n",
    "position_terms = dict()\n",
    "terms = list(Inverted_Positional_Dictionary.keys())\n",
    "K = len(Inverted_Positional_Dictionary)\n",
    "for i in range(0,len(terms)):\n",
    "    position_terms[terms[i]] = i\n",
    "p = list()\n",
    "for i in range(0,1000):\n",
    "    p.append(list())\n",
    "for i in range(0,1000):\n",
    "    p[i] = vectorizedoc(K,i,position_terms,Inverted_Positional_Dictionary,Document_term_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Leaders.pkl','rb') as f:\n",
    "    Leader_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinesim(a,b):\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    cos = dot / (norma * normb)\n",
    "    return cos\n",
    "def assign_followers(p,L):\n",
    "    Leader_followed = np.zeros(1000)\n",
    "    Leader_foll_dict = dict()\n",
    "    for i in range(0,1000):\n",
    "        maxsim = 0\n",
    "        for j in L:\n",
    "            if(cosinesim(p[i],p[j])>maxsim):\n",
    "                maxsim = cosinesim(p[i],p[j])\n",
    "                Leader_followed[i] = int(j)\n",
    "    Leader_followed = Leader_followed.astype(int)\n",
    "    for i in L:\n",
    "        Leader_foll_dict[i] = list()\n",
    "    for i in range(0,1000):\n",
    "        Leader_foll_dict[Leader_followed[i]].append(i)\n",
    "    return Leader_foll_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Leader_follow_list = assign_followers(p,Leader_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
